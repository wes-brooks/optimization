---
webr:
  packages: ['ggplot2', 'dplyr']
---
{{< include _webr-include.qmd >}}

# Types of optimization problems

There are two broad kinds of optimization problems that you are likely to encounter: those with continuous parameters, and those with discrete parameters.

## Continuous parameters

In the case of continuous parameters (like the gravity example before), the basic task is to where the derivative of loss with respect to the parameters is zero. Most (all?) algorithms will find a *local* minimum, with no guarantee that it finds the global minimum. There are two basic categories of optimizer for continuous parameters: *closed form* and *iterative*. A closed form optimizer is a special solution to a specific problem, and generally can't be applied to a different problem. So, most (all?) generic optimizers are iterative: they progress toward the solution in repeated steps until the steps get small enough that we say the algorithm has *converged*.

Next, we can break optimization algorithms into categories based on how much information we have about the derivatives of the function w.r.t. the parameters. The most common categories are to have:
0. No knowledge of the derivatives.
1. Known first derivative.
2. Known first and second derivatives.



Here is an example:

```{webr grad-descent-setup-grid, echo=FALSE}
source("prepare_grid_plots.R")
```


```{webr plot-newton-raphson-grid}
# first pass:
drawIt(gplot_nr)

# second pass:
res = iterate_nr(gplot_nr, x_last = 2.5)
drawIt(res[["plot"]])

# third and sbsequent passes:
res = iterate_nr(res[["plot"]], x_last = res[["x_new"]])
drawIt(res[["plot"]])
```


```{webr plot-grad-descent-grid}
# first pass:
drawIt(gplot_gd)

# second pass:
result = iterate_gd(gplot_gd, gamma = 0.005, x_last = 2.5)
drawIt(result[["plot"]])

# third and sbsequent passes:
res = iterate_gd(res[["plot"]], gamma = result[["gamma"]], x_last = res[["x_new"]])
drawIt(res[["plot"]])
```

