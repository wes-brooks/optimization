[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Function optimization",
    "section": "",
    "text": "Overview\nOptimization here is understood to mean function optimization, which is finding the parameters that minimize or maximize a function. This is how parameters are estimated - for instance, you might observe the motion of some falling objects and use that data to estimate the constant of gravitational acceleration. In this example, you’d start with an equation derived from theory, like: \\[ y(t) = \\frac{gt^2}{2} \\] plus some data (observations of \\(y\\) and \\(t\\)), then use optimization to find the value of \\(g\\) that fits the data best.\nWhat does it mean to “fit the data best”? In this case, you’d probably use a measure of error that can handle errors to both positive and negative directions, like squared error (\\(\\sum_{i=1}^n(y_i - \\frac{gt_i^2}{2})^2\\)) or absolute error (\\(\\sum_{i=1}^n|y_i - \\frac{gt_i^2}{2}|\\)). Then you would minimize the error. This minimization (or maximization) is what optimization is all about."
  },
  {
    "objectID": "index.html#webr-example",
    "href": "index.html#webr-example",
    "title": "Function optimization",
    "section": "WebR example",
    "text": "WebR example\n\n\nLoading webR…"
  },
  {
    "objectID": "01_types.html#continuous-parameters",
    "href": "01_types.html#continuous-parameters",
    "title": "Types of optimization problems",
    "section": "Continuous parameters",
    "text": "Continuous parameters\nIn the case of continuous parameters (like the gravity example before), the basic task is to where the derivative of loss with respect to the parameters is zero. Most (all?) algorithms will find a local minimum, with no guarantee that it finds the global minimum. There are two basic categories of optimizer for continuous parameters: closed form and iterative. A closed form optimizer is a special solution to a specific problem, and generally can’t be applied to a different problem. So, most (all?) generic optimizers are iterative: they progress toward the solution in repeated steps until the steps get small enough that we say the algorithm has converged.\nNext, we can break optimization algorithms into categories based on how much information we have about the derivatives of the function w.r.t. the parameters. The most common categories are to have: 0. No knowledge of the derivatives. 1. Known first derivative. 2. Known first and second derivatives.\nHere is an example:\n\n\nLoading webR…\n\n\n\n\n\n\n\n\n\n\nNewton-Raphson\nThis algorithm is for the case that you know first and second derivatives of the function you are optimizing. It is also important that the function be convex, or the solution may not be found.\nNewton-Raphson optimization takes the Taylor expansion of the function to two terms, which means approximating the function as a parabola starting from some starting point. The minimum of a parabola is easy to find analytically (closed form), so we jump to that point and iterate the process. This makes more sense if you see it in action:\n\n\nLoading webR…\n\n\n\n\n\n\n\n\n\nLet’s look now at an example of a nonconvex function:\n\n\nLoading webR…\n\n\n\n\n\n\n\n\n\nNewton-Raphson is very fast because each step is closed-form. But because the Newton-Raphson algorithm requires evaluating to the second derivative, it can be very slow to optimize a high-dimensional problem (remember that you need to evaluate cross-derivatives of a vector function, so that’s \\(p^2\\) derivatives for \\(p\\) parameters - but even wouldn’t be so bad if you didn’t then need to invert the second-derivative matrix. Truly nasty stuff. I’ve crashed clusters this way.)\n\n\nGradient descent\nWhen the second derivatives are unknown or impractical, gradient descent may work better. For gradient descent, you ony need the first derivative w.r.t. each parameter, then take a step in the direction of greatest improvement in the objective. This is simple but not so fast as Newton-Raphson because the size of each step must be tuned by a loop. Still, it is fast and simple and usually the best choice when you’re in a situation of needing to call an optimizer (rather than a professionally engineered estimation function). There are many flavors of gradient descent to improve its speed and stability, but the only one worth mentioning here is conjugate gradient descent (commonly abbreviated CG), which includes some momentum from past steps in future steps. Again, an example may help illustrate the algorithm:\n\n\nLoading webR…\n\n\n\n\n\n\n\n\n\nGradient descent is better-suited to optimizing our nonconvex function:\n\n\nLoading webR…\n\n\n\n\n\n\n\n\n\n\n\nQuick-and-dirty options\nWhen your number of parameters is small and the data size is modest, you may not have to bother with calculating derivatives. The built-in general-purpose optimization functions in R and Python default to Nelder-Mead or BFGS algorithms, which essentially make each step by trying a bunch of candidates around the current location. They are “slow” in the computer, but fast for the scientist - and only the scientist gets paid for their time.\n\n\nAutomatic differentiation\nOn the other hand, if your problem is very complicated, your easiest bet may be to use automatic differentiation and then conjugate gradient descent or Newton-Raphson. The widely used differentiation package is adcomp, aka Template Model Builder (TMB). You use it by writing your objective function as a C++ function template, and then the adcomp software hits everything with the chain rule as many ties as necessary to get derivatives. I love this but the barrier to entry is large.\n\n\nConstrained optimization\nOften, the reason to do custom optimization is because you need to constrain your solution, such as saying that all of the parameters should sum to one. Equality constraints are easy to implement via Lagrange multipliers. This is a fancy name for adding something like \\(\\lambda (\\sum_{i=1}^p \\beta_i - 1)\\) to your loss function. Since the minimum is found where the derivative of this term w.r.t. \\(\\lambda\\) is zero, the minimum is where \\(\\sum_{i=1}^p \\beta_i = 1\\). Let’s try it:\n\n\nLoading webR…\n\n\n\n\n\n\n\n\n\nNow we’ve looked at the data. Lets estimate the mass of each penguin species, with the constraint that Chinstrap and Adelie penguins have the same mass.\n\n\nLoading webR…"
  },
  {
    "objectID": "01_types.html#discrete-parameters",
    "href": "01_types.html#discrete-parameters",
    "title": "Types of optimization problems",
    "section": "Discrete parameters",
    "text": "Discrete parameters\nWhen parameters are discrete, the loss isn’t continuous, so you can’t minimize the function by finding where the derivatives w.r.t. the parameters are zero. For small problems 9like deciding which of three variables should be included in a regression model), it is easy enough to check all possible combinations and choose the best one. But to search all of the possible solutions to a large problem would be combinatorically impossible (e.g. the number of outcomes when shuffling a 52-card deck is on the order of the number of atoms in the universe). For problems like that, I’d recommend genetic algorithms. I’m not going to provide an example because they require packages outside of the base R or Python.\n\nGenetic algorithm\nThe gist of a genetic algorithm is that you define a “fitness” function (like our loss functions for continuous optiization) as well as a population of randomly selected combinations of parameters. The candidates in that population are then passed to the next generation with a probability proportional to their fitness and those that pass through are also randomly modified (aka “mutated”) by a small amount. After any generations of this process, you may end up with a good solution to the optimization problem."
  },
  {
    "objectID": "index.html#optim-example",
    "href": "index.html#optim-example",
    "title": "Function optimization",
    "section": "optim() example",
    "text": "optim() example\nYour favorite scripting/analysis software surely comes with an optimizer built in. Here, we’ll use R’s optim() to estimate g, the acceleration due to gravity. I dropped ten balls from a height of five meters and measured how long it took for them to hit the ground. Using that data, I will say that the loss is the absolute difference between the five meters of actual height and the distance that the balls should have fallen for any particular estimate of g (which is \\((g \\times t^2) / 2\\)).\n\n\nLoading webR…\n\n\n\n\n\n\n\n\n\nNow, we’ll look at how optim() does that."
  },
  {
    "objectID": "index.html#url-for-the-presentation-is-wes-brooks.github.iooptimization",
    "href": "index.html#url-for-the-presentation-is-wes-brooks.github.iooptimization",
    "title": "Function optimization",
    "section": "URL for the presentation is wes-brooks.github.io/optimization",
    "text": "URL for the presentation is wes-brooks.github.io/optimization\nOptimization here is understood to mean function optimization, which is finding the parameters that minimize or maximize a function. This is how parameters are estimated - for instance, you might observe the motion of some falling objects and use that data to estimate the constant of gravitational acceleration. In this example, you’d start with an equation derived from theory, like: \\[ y(t) = \\frac{gt^2}{2} \\] plus some data (observations of \\(y\\) and \\(t\\)), then use optimization to find the value of \\(g\\) that fits the data best.\nWhat does it mean to “fit the data best”? In this case, you’d probably use a measure of error that can handle errors to both positive and negative directions, like squared error (\\(\\sum_{i=1}^n(y_i - \\frac{gt_i^2}{2})^2\\)) or absolute error (\\(\\sum_{i=1}^n|y_i - \\frac{gt_i^2}{2}|\\)). Then you would minimize the error. This minimization (or maximization) is what optimization is all about."
  }
]